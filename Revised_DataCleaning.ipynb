{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('data_clean_and_preprocess').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"sep\",\"\\t\").csv(\"column_headers.tsv\",header='true', inferSchema='true')\n",
    "df1 = spark.read.option(\"sep\",\"\\t\").csv(\"hit_data.tsv\",header='false', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as sf\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlc = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.union(df1)\n",
    "#result.head(1)\n",
    "#result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the following steps are referred from the Adobe Clickstream Analytics (ACA) -page 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoValuedExcludeHit=result.filter(result.exclude_hit == 0)\n",
    "#NoValuedExcludeHit.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValidHitSourceWithout5=NoValuedExcludeHit.filter(NoValuedExcludeHit.hit_source != 5 )\n",
    "ValidHitSourceWithout7=ValidHitSourceWithout5.filter(ValidHitSourceWithout5.hit_source != 7 )\n",
    "ValidHitSourceWithout9=ValidHitSourceWithout7.filter(ValidHitSourceWithout7.hit_source != 9 )\n",
    "ValidHitSourceWithout8=ValidHitSourceWithout9.filter(ValidHitSourceWithout9.hit_source != 8 )\n",
    "#ValidHitSourceWithout8.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MergedColumns = ValidHitSourceWithout8.withColumn('joined_column', sf.concat(sf.col('post_visid_high'),sf.lit('_'), sf.col('post_visid_low')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# considering post_ columns for preprocessing ACA-page 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apart from post_ columns considering column obtained after joining the two post_ columns as mentioned above and mcvisid\n",
    "MergedColumns=MergedColumns.withColumnRenamed('joined_column', 'post_uniqueId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MergedColumns=MergedColumns.withColumnRenamed('mcvisid', 'post_mcvisid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the extraneous columns\n",
    "DropPostVHL=MergedColumns.drop('post_visid_high','post_visid_low')\n",
    "#len(DropPostVHL.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniqueId=DropPostVHL.select('post_uniqueId')\n",
    "#UniqueId.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will be used for identifying number of non-null rows\n",
    "UID=UniqueId.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering only post_ columns\n",
    "newDataF= DropPostVHL.select(*filter(lambda col:'post_' in col, DropPostVHL.columns))\n",
    "#len(newDataF.columns)\n",
    "#newDataF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing 4 data samples from the post_ subset: mobile_ , evar_, prop_, remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing post_mobile_ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post_mobile columns sub group\n",
    "mobileDF=newDataF.select(*filter(lambda col:'post_mobile' in col, newDataF.columns))\n",
    "#len(mobileDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasMobileDF=mobileDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked for null values\n",
    "import numpy as np\n",
    "PandasMobileDF.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print how many null values are there\n",
    "for column in list(PandasMobileDF.columns):\n",
    "    print (\"{}% of the data from {} column is missing\".format(round(PandasMobileDF[column].isnull().sum() * 100 / len(UID.post_uniqueId),2), column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the post_mobile_ columns because all contain null values\n",
    "Without_post_mobile=newDataF.drop(*filter(lambda col:'post_mobile' in col, newDataF.columns))\n",
    "#len(Without_post_mobile.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'post_evar' columns\n",
    "Without_post_evar=Without_post_mobile.drop(*filter(lambda col:'post_evar' in col, Without_post_mobile.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'post_prop' columns\n",
    "Without_post_prop=Without_post_evar.drop(*filter(lambda col:'post_prop' in col, Without_post_evar.columns))\n",
    "#len(Without_post_prop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "Without_post_prop.select([count(when(col(c).isNotNull(), c)).alias(c) for c in Without_post_prop.columns]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected=Without_post_prop.select(\"post_event_list\",\"post_page_event\",\"post_referrer\",\"post_visid_type\",\"post_uniqueId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing post_prop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter post_prop to find null values\n",
    "postProp=newDataF.select(*filter(lambda col:'post_prop' in col, newDataF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "postProp.select([count(when(col(c).isNull(), c)).alias(c) for c in postProp.columns]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullPostProp=newDataF.select(\"post_prop2\",\"post_prop3\",\"post_prop5\",\"post_prop6\",\"post_prop13\",\"post_prop35\",\"post_prop60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "NoNullPostPropC=NoNullPostProp.select([count(when(col(c).isNotNull(), c)).alias(c) for c in NoNullPostProp.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NoNullPostProp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in NoNullPostPropC.columns:\n",
    "#    if NoNullPostPropC[[c]].first()[c] > 1000000 :\n",
    "#        print (\" {} for {} \".format(NoNullPostPropC[[c]].first()[c],c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullPostPropPandas=NoNullPostProp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullPostPropPandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing post_evar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter eVar to find null values\n",
    "eVar=newDataF.select(*filter(lambda col:'post_evar' in col, newDataF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for non-null values in eVar\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "eVar.select([count(when(col(c).isNotNull(), c)).alias(c) for c in eVar.columns]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "eVarC=eVar.select([count(when(col(c).isNotNull(), c)).alias(c) for c in eVar.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in eVarC.columns:\n",
    "    #if eVarC[[c]].first()[c] > 100000 :\n",
    "        #print (\" {} for {} \".format(eVarC[[c]].first()[c],c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullevar=newDataF.select( \"post_evar1\", \"post_evar2\", \"post_evar8\", \"post_evar11\", \"post_evar26\",\"post_evar27\",\"post_evar28\",\"post_evar33\",\"post_evar34\",\"post_evar35\",\"post_evar38\", \"post_evar39\", \"post_evar40\", \"post_evar47\",\"post_evar48\",\"post_evar49\",\"post_evar51\",\"post_evar53\",\"post_evar63\",\"post_evar64\",\"post_evar65\",\"post_evar68\",\"post_evar70\",\"post_evar71\", \"post_evar77\",\"post_evar78\",\"post_evar80\",\"post_evar98\",\"post_evar99\",\"post_evar101\",\"post_evar102\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "NoNullevarC=NoNullevar.select([count(when(col(c).isNotNull(), c)).alias(c) for c in NoNullevar.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in NoNullevarC.columns:\n",
    "#    if NoNullevarC[[c]].first()[c] > 1000000 :\n",
    "#        print (\" {} for {} \".format(NoNullevarC[[c]].first()[c],c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullevar=NoNullevar.select( \"post_evar27\", \"post_evar33\", \"post_evar34\", \"post_evar35\", \"post_evar39\",\"post_evar47\",\"post_evar71\",\"post_evar78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted to Pandas for viewing\n",
    "NoNullevarPandas=NoNullevar.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoNullevarPandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked for the relevance to eliminate columns having Low Variance, high correlation\n",
    "#column post_evar27 has Low Variance\n",
    "#column post_evar35 is the combination of values of columns post_evar34 and post_evar47 and high correlation\n",
    "newPandasDataF=NoNullevarPandas.drop(['post_evar27','post_evar34', 'post_evar47','post_evar33'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newPandasDataF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected the identified non-null columns with 100000 but null with 1000000 as the threshold to check the columns relevance\n",
    "#post_evar101 and post_evar102 look correlated. keeping one seems important\n",
    "newDataF.select(\"post_evar1\", \"post_evar2\", \"post_evar8\", \"post_evar11\", \"post_evar26\",\"post_evar28\",\"post_evar38\",\"post_evar40\",\"post_evar48\",\"post_evar49\",\"post_evar51\",\"post_evar53\",\"post_evar63\",\"post_evar64\",\"post_evar65\",\"post_evar68\",\"post_evar70\",\"post_evar77\",\"post_evar80\",\"post_evar98\",\"post_evar99\",\"post_evar101\",\"post_evar102\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some more relevant columns to the dataset after looking at the dropped columns after setting threshold value to 1000000\n",
    "#columns include post_evar11,post_evar48,post_evar51,post_evar53\n",
    "NoNullevar=newDataF.select( \"post_evar11\", \"post_evar35\", \"post_evar39\",\"post_evar48\",\"post_evar51\",\"post_evar53\",\"post_evar71\",\"post_evar78\",\"post_evar77\",\"post_evar102\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=NoNullevar.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing prop_, evar_ and remaining extracted relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post_uniqueId, post_evar71 and post_mcvisid look like identifiers\n",
    "#post_evar71=post_mcvisid\n",
    "selected=newDataF.select(\"post_uniqueId\",\"post_evar71\",\"post_mcvisid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what post_evar11 contains. But all three look like event lists\n",
    "selected=newDataF.select(\"post_evar11\",\"post_event_list\",\"post_evar71\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referrer is whichever link the user followed to the current page.\n",
    "#If evar51=Referrer. there will be a link in the post-referrer\n",
    "selected=newDataF.select(\"psot_evar51\",\"post_referrer\",\"post_evar53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates\n",
    "#post_prop13 == post_evar53\n",
    "selected=newDataF.select(\"post_prop13\",\"post_evar53\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates\n",
    "#post_prop6 == post_evar33\n",
    "selected=newDataF.select(\"post_prop6\",\"post_evar33\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all the relevant columns from the entire dataset\n",
    "selected=newDataF.select(\"post_event_list\",\"post_page_event\",\"post_visid_type\",\"post_referrer\",\"post_evar53\",\"post_evar71\",\"post_evar11\",\"post_prop60\",\"post_uniqueid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPandas=selected.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping By userid, events and referrer\n",
    "Dataframe=selected.groupBy(\"post_uniqueid\",\"post_event_list\",\"post_referrer\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting on unique userid\n",
    "df=Dataframe.orderBy([\"post_uniqueid\"],ascending=1)\n",
    "#df.show()\n",
    "#df.select(\"post_event_list\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting details for a particular user\n",
    "df.filter(df['post_uniqueid']==\"1658310662_925910793\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing no distinct users in an array to split-on for grouping\n",
    "listids = [list(x.asDict().values())[0] for x in df.select(\"post_uniqueid\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes lot of time to load\n",
    "dfArray = [df.where(df.post_uniqueid == x) for x in listids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting no. of distinct users\n",
    "distinct=df.select('post_uniqueid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through all unique userids\n",
    "for i in range(0,distinct):\n",
    "    dfArray[i].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying if the column contains null values\n",
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"post_event_list\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the null values from event_list\n",
    "WithoutNaEvent=df.na.drop(subset=[\"post_event_list\"])\n",
    "#WithoutNaEvent.show()\n",
    "#WithoutNaEvent.count()\n",
    "#WithoutNaEvent.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting event list containing 1's\n",
    "SelectedCols=WithoutNaEvent.filter(WithoutNaEvent.post_event_list.rlike(',1,')).select(\"post_event_list\",\"post_uniqueid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the unique ids for the event_list containing 1's\n",
    "listids = [list(x.asDict().values())[0] for x in WithoutNaEvent.filter(WithoutNaEvent.post_event_list.rlike(',1,')).select(\"post_uniqueid\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of userids containing 1's\n",
    "print(\"{}\".format(len(listids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the entire set of event_lists for the userids in the list containing 1's\n",
    "for x in listids[0:3]:\n",
    "    WithoutNaEvent.where(WithoutNaEvent.post_uniqueid == x).show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
